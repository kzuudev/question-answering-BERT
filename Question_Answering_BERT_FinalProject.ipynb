{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aJIx9GC8IbTw",
        "PnJYq08SJOfw",
        "2qL2ZF7slsvR",
        "E8zdKOR1RAB0",
        "mmLJy4pAOHTK",
        "CUL-24JbDKgd",
        "Xpl6mXprDVDe",
        "YRPf8q0LAqYA"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup and Data Preparation**"
      ],
      "metadata": {
        "id": "5qPjJmHSGryK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function Description**\n",
        "\n",
        "This code block loads a CSV dataset from Google Drive (in this case `(\"/content/drive/My Drive/Data Collection (ITE Elective Course Lesson)/Dataset/Webscraped data - ITE Elective 3 - Sheet1.csv\")`, then processes it into a suitable format for `transformers` libraries. It also checks GPU availability to enable efficient computation.\n",
        "\n",
        "**Input**\n",
        "\n",
        "The input in this code block is the CSV file from Google Drive containing text data for model training\n",
        "\n",
        "**Output**\n",
        "\n",
        "The output in this code block is the confirmation message about GPU or CPU usage. Two datasets printed to the console: training data `(train_data)` and evaluation data `(eval_data)`.\n",
        "\n",
        "**Essential Syntaxes**\n",
        "\n",
        "\n",
        "\n",
        "    drive.mount('/content/drive/', force_remount=True) mounts Google Drive to access external files.\n",
        "\n",
        "    pd.read_csv(path) loads the dataset into a pandas DataFrame.\n",
        "\n",
        "    torch.cuda.is_available() checks whether a GPU is accessible.\n",
        "\n",
        "    Dataset.from_pandas(df) converts a pandas DataFrame into a Hugging Face Dataset object.\n",
        "\n",
        "    dataset.train_test_split(test_size=0.2, seed=42) splits the dataset into 80% training and 20% evaluation data.\n",
        "\n",
        "    torch.device(\"cuda\" or \"cpu\") specifies the computation device for model training.\n",
        "\n",
        "**Example Output**\n",
        "\n",
        "Mounted at /content/drive/\n",
        "Using GPU: Tesla T4\n",
        "\n",
        "--- Loading and Preprocessing Data ---\n",
        "Dataset({\n",
        "    features: ['column1', 'column2', 'column3'],\n",
        "    num_rows: 400\n",
        "})\n",
        "Dataset({\n",
        "    features: ['column1', 'column2', 'column3'],\n",
        "    num_rows: 100\n",
        "})\n",
        "\n",
        "\n",
        "**Comment and Observation**\n",
        "\n",
        "Based on my understanding, the code demonstrates a typical preprocessing workflow for NLP model training using Hugging Face. It ensures GPU acceleration if available, which significantly speeds up training. Moreover, converting the dataset into the Hugging Face format allows easy integration with a `Trainer` later on."
      ],
      "metadata": {
        "id": "LDkbIboypqOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/Data Collection/Dataset_QnA.xlsx\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Opkq-lNVLbmF",
        "outputId": "78d1470c-e82e-48b8-af00-fcd351ed35ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/drive/My Drive/Data Collection/Dataset_QnA.xlsx': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOQpViEWvJcM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "1261742e-0f24-48a8-b0a2-aef6635b8257"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-803637871.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfingerprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisable_caching\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_caching\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_caching_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetInfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m from .inspect import (\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mget_dataset_config_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mget_dataset_config_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/inspect.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming_download_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStreamingDownloadManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetInfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m from .load import (\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mdataset_module_factory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mget_dataset_builder_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0miterable_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterableDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnaming\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcamelcase_to_snakecase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnakecase_to_camelcase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m from .packaged_modules import (\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0m_EXTENSION_TO_MODULE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0m_MODULE_TO_EXTENSIONS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/packaged_modules/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minsecure_hashlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maudiofolder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudiofolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/packaged_modules/arrow/arrow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import itertools\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from google.colab import drive\n",
        "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers.data.data_collator import default_data_collator # Import default_data_collator\n",
        "\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# df = pd.read_excel(\"/content/drive/My Drive/Data Collection (ITE Elective Course Lesson)/Dataset/Webscraped data_Modules_Question and Answering.xlsx\")\n",
        "# df = pd.read_excel(\"/content/drive/My Drive/Data Collection (ITE Elective Course Lesson)/Dataset/Webscraped data_Modules_Question and Answering_Revised.xlsx\")\n",
        "df = pd.read_excel(\"/content/drive/My Drive/Data Collection/Dataset_QnA.xlsx\")\n",
        "\n",
        "# Clean column names by stripping whitespace and newline characters\n",
        "df.columns = [col.strip().replace('\\n', '') for col in df.columns]\n",
        "\n",
        "# Print column names to debug KeyError\n",
        "print(\"DataFrame columns:\", df.columns.tolist())\n",
        "\n",
        "# Diagnostic prints before dropna\n",
        "print(f\"DataFrame shape before dropna: {df.shape}\")\n",
        "print(\"NaN counts before dropna (in specified subset columns):\")\n",
        "print(df[['Title', 'Context', 'Exam Question', 'Exam Answer', 'Quiz Question', 'Quiz Answer']].isnull().sum())\n",
        "\n",
        "df.dropna (subset=['Title', 'Context', 'Exam Question', 'Exam Answer', 'Quiz Question', 'Quiz Answer'], inplace=True)\n",
        "\n",
        "# Diagnostic print after dropna\n",
        "print(f\"DataFrame shape after dropna: {df.shape}\")\n",
        "\n",
        "\n",
        "for col in ['Context', 'Exam Question', 'Exam Answer', 'Quiz Question', 'Quiz Answer', 'Title']:\n",
        "  df[col] = df[col].astype(str)\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^A-Za-z0-9.,;:?!\\\"()\\-\\s]', '', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "df['Context'] = df['Context'].apply(clean_text)\n",
        "df['Exam Question'] = df['Exam Question'].apply(clean_text)\n",
        "df['Exam Answer'] = df['Exam Answer'].apply(clean_text)\n",
        "df['Quiz Question'] = df['Quiz Question'].apply(clean_text)\n",
        "df['Quiz Answer'] = df['Quiz Answer'].apply(clean_text)\n",
        "\n",
        "print(f\"Cleaned dataset shape: {df.shape}\")\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, using CPU.\")\n",
        "\n",
        "print(\"\\n--- Loading and Preprocessing Data ---\")\n",
        "\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_data = dataset[\"train\"]\n",
        "eval_data = dataset[\"test\"]\n",
        "\n",
        "print(train_data)\n",
        "print(eval_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bert-base-uncased**"
      ],
      "metadata": {
        "id": "aJIx9GC8IbTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function Description**\n",
        "\n",
        "This code loads a pre-trained BERT model and tokenizer for question answering tasks, processes datasets by aligning answer positions within contexts, and tokenizes the data while mapping character-based answer spans to token spans suitable for model training. It also detects GPU availability to accelerate model training and inference.\n",
        "\n",
        "**Input**\n",
        "\n",
        "The input consists of datasets containing questions, contexts, and answers, which are prepared as train_data and eval_data before processing. These datasets include fields like \"Question,\" \"Context,\" \"Answer,\" and answer positions.\n",
        "\n",
        "**Output**\n",
        "\n",
        "The output includes tokenized datasets (tokenized_train and tokenized_eval) that are formatted for PyTorch training, with added start and end position labels for answers. Additionally, the script confirms the successful loading of the model and the availability of GPU acceleration, printing relevant messages.\n",
        "\n",
        "**Essential Syntaxes**\n",
        "\n",
        "python\n",
        "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
        "\n",
        "    Imports the necessary classes for tokenization and model loading.\n",
        "\n",
        "python\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "model = BertForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    Loads a pre-trained BERT tokenizer and model based on bert-base-uncased.\n",
        "\n",
        "python\n",
        "def add_answer_positions(example):\n",
        "    ...\n",
        "    return example\n",
        "\n",
        "    Adds start and end answer positions within the context, matching answer text to context.\n",
        "\n",
        "python\n",
        "def tokenize_and_align(examples):\n",
        "    ...\n",
        "    return tokenized\n",
        "\n",
        "    Tokenizes question and context, aligns answer spans with token indices, and prepares data for model training.\n",
        "\n",
        "python\n",
        "train_data.map(add_answer_positions)\n",
        "eval_data.map(add_answer_positions)\n",
        "\n",
        "train_data.map(tokenize_and_align, batched=True)\n",
        "eval_data.map(tokenize_and_align, batched=True)\n",
        "\n",
        "    Applies position setting and tokenization functions on datasets.\n",
        "\n",
        "python\n",
        "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    Checks for GPU availability and assigns the device accordingly.\n",
        "\n",
        "python\n",
        "model = BertForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)\n",
        "\n",
        "    Loads the model to the appropriate device for training or inference.\n",
        "\n",
        "Example Output\n",
        "\n",
        "text\n",
        "Loaded Pretrained QnA Model: bert-base-uncased\n",
        "ðŸ§© Model loaded successfully for QnA: bert-base-uncased\n",
        "\n",
        "Comment and Observation\n",
        "\n",
        "This code exemplifies a typical NLP pipeline for question answering with BERT. It carefully maps answer spans from character-level positions within the context to token indices, which is essential for model training. Importantly, it checks for GPU availability to optimize performance, making it suitable for large datasets and neural network fine-tuning tasks. The structured approach ensures correct data alignment and efficient model loading, facilitating smooth integration into a training loop later on."
      ],
      "metadata": {
        "id": "D0pP3KAAQZbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "model = BertForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(f\"\\n Loaded Pretrained QnA Model: {MODEL_NAME}\")\n",
        "\n",
        "\n",
        "def add_answer_positions(example):\n",
        "    context = example[\"Context\"]\n",
        "    quiz_answer = example[\"Quiz Answer\"]\n",
        "    exam_answer = example[\"Exam Answer\"]\n",
        "\n",
        "\n",
        "    context_lower = context.lower()\n",
        "    quiz_answer_lower = quiz_answer.lower()\n",
        "    exam_answer_lower = exam_answer.lower()\n",
        "\n",
        "    quiz_answer_start = context_lower.find(quiz_answer_lower)\n",
        "    exam_answer_start = context_lower.find(exam_answer_lower)\n",
        "    if quiz_answer_start & exam_answer_start == 0:\n",
        "\n",
        "        example[\"start_positions\"] = 0\n",
        "        example[\"end_positions\"] = 0\n",
        "    else:\n",
        "        answer_end = answer_start + len(answer)\n",
        "        example[\"start_positions\"] = answer_start\n",
        "        example[\"end_positions\"] = answer_end\n",
        "\n",
        "    return example\n",
        "\n",
        "\n",
        "def tokenize_and_align(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"Quiz Question\"],\n",
        "        examples[\"Exam Question\"],\n",
        "        examples[\"Context\"],\n",
        "        truncation=\"only_second\",\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(tokenized[\"offset_mapping\"]):\n",
        "        sequence_ids = tokenized.sequence_ids(i)\n",
        "        context_start = sequence_ids.index(1)\n",
        "        context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n",
        "\n",
        "        start_char = examples[\"start_positions\"][i]\n",
        "        end_char = examples[\"end_positions\"][i]\n",
        "\n",
        "\n",
        "        token_start_index = context_start\n",
        "        token_end_index = context_start\n",
        "\n",
        "\n",
        "        for idx in range(context_start, context_end + 1):\n",
        "            if offsets[idx][0] <= start_char < offsets[idx][1]:\n",
        "                token_start_index = idx\n",
        "            if offsets[idx][0] < end_char <= offsets[idx][1]:\n",
        "                token_end_index = idx\n",
        "                break\n",
        "\n",
        "        start_positions.append(token_start_index)\n",
        "        end_positions.append(token_end_index)\n",
        "\n",
        "    tokenized[\"start_positions\"] = start_positions\n",
        "    tokenized[\"end_positions\"] = end_positions\n",
        "    tokenized.pop(\"offset_mapping\")\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "\n",
        "train_data = train_data.map(add_answer_positions)\n",
        "eval_data = eval_data.map(add_answer_positions)\n",
        "\n",
        "\n",
        "tokenized_train = train_data.map(tokenize_and_align, batched=True)\n",
        "tokenized_eval = eval_data.map(tokenize_and_align, batched=True)\n",
        "\n",
        "tokenized_train.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
        "tokenized_eval.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)\n",
        "\n",
        "print(f\"\\n Model loaded successfully for QnA: {MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "lkp9RR_6yOSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "orJ1vJMTKVzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **METRICS AND TRAINING SETUP**"
      ],
      "metadata": {
        "id": "PnJYq08SJOfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command installs several important Python packages that are essential for working with transformer models and hyperparameter optimization:\n",
        "\n",
        "    transformers: Hugging Face's library for state-of-the-art transformer models like BERT, GPT, and more.\n",
        "\n",
        "    datasets: Hugging Face's library for easily accessing and managing datasets.\n",
        "\n",
        "    accelerate: A library to help scale PyTorch models across multiple GPUs or TPUs.\n",
        "\n",
        "    ray[tune]: Ray Tune is a scalable hyperparameter tuning library built on Ray.\n",
        "\n",
        "    optuna: A popular framework for automated hyperparameter optimization.\n",
        "\n",
        "The purpose of this code is to search results show that Optuna and Ray Tune can be integrated with the Transformers Trainer to perform hyperparameter searches efficiently. Installing these packages sets up your environment for such advanced workflows, enabling you to automatically and systematically improve your transformer modelâ€™s training parameters."
      ],
      "metadata": {
        "id": "oSfvOpUqRHW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate ray[tune] optuna -U\n",
        "\n"
      ],
      "metadata": {
        "id": "ZE-ubxrUkUOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function Description**\n",
        "\n",
        "This code sets seeds for all relevant libraries to ensure full reproducibility in PyTorch experiments. It also defines evaluation metrics for question answering Exact Match (EM) and F1 Score then builds a Hugging Face Trainer with arguments perfectly suited for BERT-style extractive QA training and validation. These steps are essential for stable, trackable QA research and applications.\n",
        "\n",
        "**Input**\n",
        "\n",
        "    Seed value (here, 42), set across Python, numpy, PyTorch CPU and GPU.\n",
        "\n",
        "    Model predictions and gold answers for each evaluation batch.\n",
        "\n",
        "    Training datasets, pretrained model, and tokenizer.\n",
        "\n",
        "**Output**\n",
        "\n",
        "    Reproducible training and validation runs.\n",
        "\n",
        "    Dictionary of computed evaluation metrics (averaged EM, F1, and inference time) after each evaluation phase.\n",
        "\n",
        "    A fully configured Trainer object for orchestrated fine-tuning on QA tasks.\n",
        "\n",
        "**Essential Syntaxes**\n",
        "\n",
        "python\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "Ensures all sources of randomness are controlled, making repeated experiments yield consistent results.\n",
        "\n",
        "python\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(prediction.strip().lower() == truth.strip().lower())\n",
        "\n",
        "Computes an all-or-nothing score: 1 only if predicted span matches gold answer exactly, after stripping and lowercasing.\n",
        "\n",
        "python\n",
        "def compute_f1(prediction, truth):\n",
        "    pred_tokens = prediction.lower().split()\n",
        "    truth_tokens = truth.lower().split()\n",
        "    ...\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "Calculates the token-level overlap F1 between prediction and gold answer. Captures partial matches as well as perfect ones.\n",
        "\n",
        "python\n",
        "def compute_metrics(eval_pred):\n",
        "    ...\n",
        "    return metrics\n",
        "\n",
        "Processes batched model predictions, decodes answer spans, computes EM and F1 across all examples, and averages them.\n",
        "\n",
        "python\n",
        "training_args = TrainingArguments(...)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "Sets up training parameters (epochs, batch sizes, device, logging, saving) and bundles everything in a Trainer for easy use.\n",
        "\n",
        "**Example Output**\n",
        "\n",
        "text\n",
        "{'Exact_Match': 0.76, 'F1_Score': 0.81, 'Avg_Inference_Time': 0.0052}\n",
        "\n",
        "(A dictionary summarizing model accuracy and efficiency, printed after evaluation.)\n",
        "\n",
        "**Comment and Observation**\n",
        "\n",
        "Setting all seeds eliminates nearly all nondeterminismâ€”so you're not chasing \"lucky runs,\" which is essential in QA research."
      ],
      "metadata": {
        "id": "-Q1g705kuwek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seed_value = 42\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(prediction.strip().lower() == truth.strip().lower())\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "    pred_tokens = prediction.lower().split()\n",
        "    truth_tokens = truth.lower().split()\n",
        "\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    if not common:\n",
        "        return 0.0\n",
        "\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(truth_tokens)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    start_time = time.time()\n",
        "\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    start_logits, end_logits = predictions\n",
        "\n",
        "\n",
        "    start_positions = np.argmax(start_logits, axis=1)\n",
        "    end_positions = np.argmax(end_logits, axis=1)\n",
        "\n",
        "    exact_matches = []\n",
        "    f1_scores = []\n",
        "\n",
        "\n",
        "    for i in range(len(start_positions)):\n",
        "        input_ids = tokenized_eval[i][\"input_ids\"]\n",
        "        pred_tokens = input_ids[start_positions[i]: end_positions[i] + 1]\n",
        "        pred_text = tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
        "\n",
        "        gold_text = eval_data[i][\"Answer\"]\n",
        "\n",
        "        exact_matches.append(compute_exact_match(pred_text, gold_text))\n",
        "        f1_scores.append(compute_f1(pred_text, gold_text))\n",
        "\n",
        "    avg_inference_time = (time.time() - start_time) / len(start_positions)\n",
        "\n",
        "    metrics = {\n",
        "        \"Exact_Match\": np.mean(exact_matches),\n",
        "        \"F1_Score\": np.mean(f1_scores),\n",
        "        \"Avg_Inference_Time\": avg_inference_time\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/My Drive/results\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"/content/drive/My Drive/logs\",\n",
        "    learning_rate=4e-5,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=[],\n",
        "    seed=42,\n",
        "    data_seed=42\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "eJPYKdMNz2r8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **With Grid Search**"
      ],
      "metadata": {
        "id": "2qL2ZF7slsvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function Description\n",
        "\n",
        "This code establishes a reproducible environment for training a BERT-based question answering model, defines evaluation metrics (Exact Match and F1), sets hyperparameters and training arguments for model fine-tuning, and implements an automated hyperparameter grid search using Optuna integrated with Hugging Faceâ€™s Trainer. After the search, it outputs the best hyperparameters and prints results from all trials for detailed analysis.\n",
        "\n",
        "**Input**\n",
        "\n",
        "    Tokenized train and evaluation datasets (tokenized_train and tokenized_eval) with questions, contexts, and answers.\n",
        "\n",
        "    Pretrained model and tokenizer for BERT-based question answering.\n",
        "\n",
        "    Defined hyperparameter search space including learning rate, batch size, and number of epochs.\n",
        "\n",
        "    Utility functions for computing Exact Match and F1 evaluation metrics.\n",
        "\n",
        "    A seed value (42) to ensure deterministic and reproducible training runs.\n",
        "\n",
        "**Output**\n",
        "\n",
        "    Metric results for each hyperparameter trial during grid search.\n",
        "\n",
        "    The best hyperparameter configuration found according to the evaluation (maximizing F1 Score).\n",
        "\n",
        "    A DataFrame printed to the console with all trial results sorted by performance metric.\n",
        "\n",
        "    Messages indicating the progress and results of the hyperparameter search.\n",
        "\n",
        "**Essential Syntaxes**\n",
        "\n",
        "python\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    Seeds different RNGs and configures CUDA backend for reproducible results.\n",
        "\n",
        "python\n",
        "def compute_exact_match(prediction, truth):\n",
        "    ...\n",
        "def compute_f1(prediction, truth):\n",
        "    ...\n",
        "def compute_metrics(eval_pred):\n",
        "    ...\n",
        "\n",
        "    Define evaluation metric functions that compute Exact Match, F1 score, and average inference time from model predictions and ground truth answers.\n",
        "\n",
        "python\n",
        "def tune_hp(trial):\n",
        "    learning_rate = trial.suggest_categorical(\"learning_rate\", [5e-5, 3e-5, 1e-5])\n",
        "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16])\n",
        "    num_train_epochs = trial.suggest_categorical(\"num_train_epochs\", [3, 4, 5])\n",
        "    return {...}\n",
        "\n",
        "    Hyperparameter search space definition for Optuna trials.\n",
        "\n",
        "python\n",
        "def model_init():\n",
        "    return BertForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)\n",
        "\n",
        "    Function to reinitialize the model freshly for each hyperparameter trial to avoid weight contamination.\n",
        "\n",
        "python\n",
        "grid_trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=grid_training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "    Trainer setup for hyperparameter search using the reinitialization function and grid search training arguments.\n",
        "\n",
        "python\n",
        "best_trial = grid_trainer.hyperparameter_search(\n",
        "    backend=\"optuna\",\n",
        "    hp_space=tune_hp,\n",
        "    direction=\"maximize\",\n",
        "    n_trials=18,\n",
        ")\n",
        "\n",
        "    Executes Optuna-powered hyperparameter search over 18 trials, maximizing the F1 metric.\n",
        "\n",
        "python\n",
        "df_results = pd.DataFrame(trial_results)\n",
        "df_results.sort_values(\"metric_value\", ascending=False, inplace=True)\n",
        "print(df_results)\n",
        "\n",
        "    Converts trial results to a DataFrame, sorts them by performance, and prints them for more interpretable analysis.\n",
        "\n",
        "**Example Output**\n",
        "\n",
        "text\n",
        "--- Starting Grid Search ---\n",
        "[I 2025-11-08 21:15:30,000] Trial 0 finished with value: 0.76\n",
        "...\n",
        "--- Grid Search Complete ---\n",
        "BEST HYPERPARAMETERS FOUND:\n",
        "{'learning_rate': 3e-5, 'per_device_train_batch_size': 8, 'num_train_epochs': 5}\n",
        "\n",
        "Grid Search Trial Results (sorted by metric):\n",
        "   learning_rate  per_device_train_batch_size  num_train_epochs  metric_value\n",
        "1        3e-05                           8                5          0.78\n",
        "0        5e-05                          16                4          0.76\n",
        "...\n",
        "\n",
        "Best hyperparameters detail:\n",
        "learning_rate                3e-05\n",
        "per_device_train_batch_size      8\n",
        "num_train_epochs                5\n",
        "metric_value                 0.78\n",
        "Name: 1, dtype: object\n",
        "\n",
        "**Comment and Observation**\n",
        "\n",
        "This code robustly integrates reproducibility best practices with automatic hyperparameter tuning for transformer QA modeling, using Optunaâ€™s search capabilities tightly coupled with Hugging Face Trainer. The use of a fresh model initialization per trial ensures unbiased evaluation of each hyperparameter set."
      ],
      "metadata": {
        "id": "iwP0-y9Qmo7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "from transformers import TrainingArguments, Trainer, set_seed\n",
        "\n",
        "import optuna\n",
        "\n",
        "seed_value = 42\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(prediction.strip().lower() == truth.strip().lower())\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "    pred_tokens = prediction.lower().split()\n",
        "    truth_tokens = truth.lower().split()\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    if not common:\n",
        "        return 0.0\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(truth_tokens)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    start_time = time.time()\n",
        "    predictions, labels = eval_pred\n",
        "    start_logits, end_logits = predictions\n",
        "    start_positions = np.argmax(start_logits, axis=1)\n",
        "    end_positions = np.argmax(end_logits, axis=1)\n",
        "    exact_matches = []\n",
        "    f1_scores = []\n",
        "    for i in range(len(start_positions)):\n",
        "        input_ids = tokenized_eval[i][\"input_ids\"]\n",
        "        pred_tokens = input_ids[start_positions[i]: end_positions[i] + 1]\n",
        "        pred_text = tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
        "        gold_text = eval_data[i][\"Answer\"]\n",
        "        exact_matches.append(compute_exact_match(pred_text, gold_text))\n",
        "        f1_scores.append(compute_f1(pred_text, gold_text))\n",
        "    avg_inference_time = (time.time() - start_time) / len(start_positions)\n",
        "    metrics = {\n",
        "        \"Exact_Match\": np.mean(exact_matches),\n",
        "        \"F1_Score\": np.mean(f1_scores),\n",
        "        \"Avg_Inference_Time\": avg_inference_time\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Define training arguments (hyperparameters)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/My Drive/results\", # Changed to Google Drive\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"/content/drive/My Drive/logs\", # Changed to Google Drive\n",
        "    learning_rate=4e-5,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=[],\n",
        "    seed=42,\n",
        "    data_seed=42\n",
        ")\n",
        "\n",
        "\n",
        "def tune_hp(trial):\n",
        "    learning_rate = trial.suggest_categorical(\"learning_rate\", [5e-5, 3e-5, 1e-5])\n",
        "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16])\n",
        "    num_train_epochs = trial.suggest_categorical(\"num_train_epochs\", [3, 4, 5])  # New hyperparameter for epochs\n",
        "    return {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"per_device_train_batch_size\": per_device_train_batch_size,\n",
        "        \"num_train_epochs\": num_train_epochs,\n",
        "    }\n",
        "\n",
        "grid_training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/My Drive/grid_search_results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    load_best_model_at_end=False,\n",
        "    metric_for_best_model=\"eval_F1_Score\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        "    num_train_epochs=3,\n",
        "    warmup_steps=500,\n",
        "    logging_dir=\"/content/drive/My Drive/grid_search_logs\",\n",
        ")\n",
        "\n",
        "def model_init():\n",
        "\n",
        "    return BertForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)\n",
        "\n",
        "grid_trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=grid_training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "print(\"\\n--- Starting Grid Search ---\")\n",
        "best_trial = grid_trainer.hyperparameter_search(\n",
        "    backend=\"optuna\",\n",
        "    hp_space=tune_hp,\n",
        "    direction=\"maximize\",\n",
        "    n_trials=18,\n",
        ")\n",
        "\n",
        "if best_trial:\n",
        "    print(\"\\n--- Grid Search Complete ---\")\n",
        "    print(\"BEST HYPERPARAMETERS FOUND:\")\n",
        "    print(best_trial)\n",
        "    best_hps = best_trial.hyperparameters\n",
        "    print(\"\\nBest Hyperparameters:\")\n",
        "    for key, val in best_hps.items():\n",
        "        print(f\"  {key}: {val}\")\n",
        "else:\n",
        "    print(\"Search failed or no best trial found.\")\n",
        "\n",
        "print(\"\\nYou can now initialize TrainingArguments with best_hps for final training.\")\n",
        "\n",
        "\n",
        "\n",
        "if grid_trainer.hp_search_backend and grid_trainer.hp_search_backend.study:\n",
        "    all_trials = grid_trainer.hp_search_backend.study.get_trials()\n",
        "\n",
        "\n",
        "    trial_results = []\n",
        "    for trial in all_trials:\n",
        "        vals = trial.params.copy()\n",
        "\n",
        "        vals['metric_value'] = trial.value\n",
        "        trial_results.append(vals)\n",
        "\n",
        "    df_results = pd.DataFrame(trial_results)\n",
        "\n",
        "\n",
        "    df_results.sort_values(\"metric_value\", ascending=False, inplace=True)\n",
        "\n",
        "    print(\"\\nGrid Search Trial Results (sorted by metric):\")\n",
        "    print(df_results)\n",
        "\n",
        "\n",
        "    print(\"\\nBest hyperparameters detail:\")\n",
        "    print(df_results.iloc[0])\n",
        "else:\n",
        "    print(\"\\nCould not retrieve trial results from the study.\")"
      ],
      "metadata": {
        "id": "j1taXROhlwKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **With Random Search**"
      ],
      "metadata": {
        "id": "E8zdKOR1RAB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function Description**\n",
        "\n",
        "This code performs a random hyperparameter search for fine-tuning a BERT question answering model using Hugging Faceâ€™s Trainer API. It sets global reproducibility seeds, defines evaluation metrics (Exact Match and F1), and iteratively trains multiple models with randomly chosen hyperparameters (learning rate, batch size, epochs). Each trial trains a fresh model instance, evaluates performance, and stores results for later analysis.\n",
        "\n",
        "**Input**\n",
        "\n",
        "    Tokenized training and evaluation datasets (tokenized_train and tokenized_eval).\n",
        "\n",
        "    Pretrained BERT question answering model and its corresponding tokenizer.\n",
        "\n",
        "    Defined search space for three hyperparameters: learning rate, batch size, and number of training epochs.\n",
        "\n",
        "    Seed value for deterministic behavior and reproducibility.\n",
        "\n",
        "**Output**\n",
        "\n",
        "    Training and evaluation metrics (Exact Match, F1 Score, average inference time) for each hyperparameter trial.\n",
        "\n",
        "    Training time per trial.\n",
        "\n",
        "    A sorted pandas DataFrame showing results of all trials ranked by F1 score.\n",
        "\n",
        "    Display of the best-performing hyperparameter combination identified from the random search.\n",
        "\n",
        "**Essential Syntaxes**\n",
        "\n",
        "python\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    Sets global random seeds and CUDA deterministic behavior to ensure reproducible results.\n",
        "\n",
        "python\n",
        "def compute_metrics(eval_pred):\n",
        "    ...\n",
        "    return metrics\n",
        "\n",
        "    Defines the evaluation function computing Exact Match, F1 Score, and average inference time by decoding predicted token spans back to text.\n",
        "\n",
        "python\n",
        "args = copy.deepcopy(base_training_args)\n",
        "args.per_device_train_batch_size = batch_size\n",
        "args.learning_rate = lr\n",
        "args.num_train_epochs = epochs\n",
        "args.seed = seed_value + trial_num\n",
        "\n",
        "    Creates an independent set of training arguments for each trial with randomized hyperparameters and a unique seed.\n",
        "\n",
        "python\n",
        "trainer = Trainer(\n",
        "    model=BertForQuestionAnswering.from_pretrained(MODEL_NAME).to(device),\n",
        "    args=args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "    Initializes a fresh Trainer for each trial with a new model instance and associated hyperparameters.\n",
        "\n",
        "python\n",
        "trainer.train()\n",
        "metrics = trainer.evaluate()\n",
        "\n",
        "    Runs training and evaluation phases for each trial.\n",
        "\n",
        "python\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results.sort_values(\"eval_f1_score\", ascending=False, inplace=True)\n",
        "\n",
        "    Aggregates trial results into a DataFrame and sorts by evaluation F1 score to identify the best trial.\n",
        "\n",
        "**Example Output**\n",
        "\n",
        "text\n",
        "Trial 4: Learning Rate=3e-05, Batch Size=16, Epochs=5\n",
        "...\n",
        "Random Search Results Sorted by F1 Score:\n",
        "   trial  learning_rate  batch_size  epochs  train_time_sec  eval_exact_match  eval_f1_score  eval_avg_inference_time\n",
        "3      4        3e-05          16       5         1189.32              0.75           0.80                  0.0051\n",
        "...\n",
        "Best Hyperparameters:\n",
        "trial                  4\n",
        "learning_rate       3e-05\n",
        "batch_size            16\n",
        "epochs                 5\n",
        "train_time_sec     1189.32\n",
        "eval_exact_match    0.75\n",
        "eval_f1_score       0.80\n",
        "eval_avg_inference_time 0.0051\n",
        "Name: 3, dtype: object\n",
        "\n",
        "Comment and Observation\n",
        "\n",
        "This implementation follows a straightforward randomized hyperparameter search strategy, ensuring each trial is statistically independent by reseeding and initializing a new model."
      ],
      "metadata": {
        "id": "EkWGThV8nOq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "from transformers import TrainingArguments, Trainer, set_seed\n",
        "from transformers.data.data_collator import default_data_collator\n",
        "from transformers import BertForQuestionAnswering  # Make sure you import your model\n",
        "import optuna\n",
        "import copy\n",
        "\n",
        "seed_value = 42\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(prediction.strip().lower() == truth.strip().lower())\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "    pred_tokens = prediction.lower().split()\n",
        "    truth_tokens = truth.lower().split()\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    if not common:\n",
        "        return 0.0\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(truth_tokens)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    start_time = time.time()\n",
        "    predictions, labels = eval_pred\n",
        "    start_logits, end_logits = predictions\n",
        "    start_positions = np.argmax(start_logits, axis=1)\n",
        "    end_positions = np.argmax(end_logits, axis=1)\n",
        "    exact_matches = []\n",
        "    f1_scores = []\n",
        "    for i in range(len(start_positions)):\n",
        "        input_ids = tokenized_eval[i][\"input_ids\"]\n",
        "        pred_tokens = input_ids[start_positions[i]: end_positions[i] + 1]\n",
        "        pred_text = tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
        "        gold_text = eval_data[i][\"Answer\"]\n",
        "        exact_matches.append(compute_exact_match(pred_text, gold_text))\n",
        "        f1_scores.append(compute_f1(pred_text, gold_text))\n",
        "    avg_inference_time = (time.time() - start_time) / len(start_positions)\n",
        "    metrics = {\n",
        "        \"Exact_Match\": np.mean(exact_matches),\n",
        "        \"F1_Score\": np.mean(f1_scores),\n",
        "        \"Avg_Inference_Time\": avg_inference_time\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "base_training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/My Drive/random_search_results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"/content/drive/My Drive/random_search_logs\",\n",
        "    learning_rate=4e-5,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    load_best_model_at_end=False,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=[],\n",
        "\n",
        "    seed=42,\n",
        "    data_seed=42,\n",
        ")\n",
        "\n",
        "\n",
        "random.seed(seed_value)\n",
        "\n",
        "num_trials = 18\n",
        "results = []\n",
        "\n",
        "for trial_num in range(1, num_trials + 1):\n",
        "\n",
        "    lr = random.choice([5e-5, 3e-5, 1e-5])\n",
        "    batch_size = random.choice([8, 16])\n",
        "    epochs = random.choice([3, 4, 5])\n",
        "\n",
        "    print(f\"\\nTrial {trial_num}: Learning Rate={lr}, Batch Size={batch_size}, Epochs={epochs}\")\n",
        "\n",
        "\n",
        "    args = copy.deepcopy(base_training_args)\n",
        "    args.per_device_train_batch_size = batch_size\n",
        "    args.per_device_eval_batch_size = batch_size\n",
        "    args.learning_rate = lr\n",
        "    args.num_train_epochs = epochs\n",
        "    args.output_dir = f\"/content/drive/My Drive/random_search_results/trial_{trial_num}\"\n",
        "\n",
        "    args.seed = seed_value + trial_num\n",
        "    args.data_seed = seed_value + trial_num\n",
        "\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=BertForQuestionAnswering.from_pretrained(MODEL_NAME).to(device),\n",
        "        args=args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_eval,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=default_data_collator,\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    results.append({\n",
        "        \"trial\": trial_num,\n",
        "        \"learning_rate\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs,\n",
        "        \"train_time_sec\": train_time,\n",
        "        \"eval_exact_match\": metrics.get(\"eval_Exact_Match\"),\n",
        "        \"eval_f1_score\": metrics.get(\"eval_F1_Score\"),\n",
        "        \"eval_avg_inference_time\": metrics.get(\"eval_Avg_Inference_Time\"),\n",
        "    })\n",
        "\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results.sort_values(\"eval_f1_score\", ascending=False, inplace=True)\n",
        "\n",
        "print(\"\\nRandom Search Results Sorted by F1 Score:\")\n",
        "display(df_results)\n",
        "\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "display(df_results.iloc[0])"
      ],
      "metadata": {
        "id": "pC7OT3k3RDqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Execution (Evaluation)**"
      ],
      "metadata": {
        "id": "dWy6J1HBJVYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The First Execution (From Previous Activity)**"
      ],
      "metadata": {
        "id": "mmLJy4pAOHTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function Description**\n",
        "\n",
        "This code block manages the full run of fine-tuning a BERT-based question answering model. It sets random seeds for reproducibility, trains the model, evaluates its performance, saves the trained model checkpoint, and communicates progress to the user.\n",
        "\n",
        "**Input**\n",
        "\n",
        "    The trainer object: A Hugging Face Trainer instance preconfigured with model, datasets, hyperparameters, metrics, and tokenizer.\n",
        "\n",
        "    Model saving path: './bert_qa_best' for checkpoint storage.\n",
        "\n",
        "**Output**\n",
        "\n",
        "    Console messages detailing the start of training, evaluation results, and completion status.\n",
        "\n",
        "    Printed evaluation metrics such as F1 Score and Exact Match after training.\n",
        "\n",
        "    Model checkpoint saved to disk for future inference tasks.\n",
        "\n",
        "**Essential Syntaxes**\n",
        "\n",
        "python\n",
        "set_seed(42)\n",
        "\n",
        "Ensures all random processes (Python, NumPy, Torch) are seeded, producing reliable and repeatable results.\n",
        "\n",
        "python\n",
        "trainer.train()\n",
        "\n",
        "Starts model fine-tuning using configured settings on your training data.\n",
        "\n",
        "python\n",
        "trainer.evaluate()\n",
        "\n",
        "Computes performance metrics on your validation or evaluation dataset.\n",
        "\n",
        "python\n",
        "trainer.save_model(\"./bert_qa_best\")\n",
        "\n",
        "Saves the trained model, tokenizer, and config to a specified directory for later use.\n",
        "\n",
        "**Example Output**\n",
        "\n",
        "--- Starting Fine-Tuning (Expected Time: 1â€“4 hours on GPU) ---\n",
        "\n",
        "[Training progress output]\n",
        "\n",
        "--- Final Evaluation Results ---\n",
        "{'eval_loss': 0.95, 'eval_Exact_Match': 0.77, 'eval_F1_Score': 0.82, ...}\n",
        "\n",
        "Fine-tuning process complete. The resulting model can now be used for Inference (Stage 4).\n",
        "\n",
        "**Comment and Observation**\n",
        "\n",
        "In this block, you see a typical workflow for preparing a BERT QA model: not just training, but also evaluating, saving, and documenting progress in clear stages."
      ],
      "metadata": {
        "id": "AFVtUKgRs22W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "\n",
        "print(\"\\n--- Starting Fine-Tuning (Expected Time: 1â€“4 hours on GPU) ---\")\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "print(\"\\n--- Final Evaluation Results ---\")\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n",
        "\n",
        "# Save the best model checkpoint for later Inference\n",
        "trainer.save_model(\"./bert_qa_best\")\n",
        "\n",
        "print(\"\\n Fine-tuning process complete. The resulting model can now be used for Inference (Stage 4).\")\n"
      ],
      "metadata": {
        "id": "aPpb02yu0_l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function Description**\n",
        "\n",
        "This code block kicks off and completes the fine-tuning process for your BERT-based question answering model using the best hyperparameters from your random search. It ensures reproducibility, performs model training, evaluates performance, saves the model checkpoint, and provides feedback for your workflow status.\n",
        "\n",
        "**Input**\n",
        "\n",
        "    An initialized trainer object (already set up with best random search hyperparameters, model, datasets, metrics, and tokenizer).\n",
        "\n",
        "    A Google Drive save path for the best checkpoint.\n",
        "\n",
        "**Output**\n",
        "\n",
        "    Progress and status messages in the console about the training, evaluation, and model saving stages.\n",
        "\n",
        "    Final evaluation metrics displayed (like F1 and Exact Match for your validation set).\n",
        "\n",
        "    A saved model checkpoint in Google Drive, ready for use during inference (Stage 4).\n",
        "\n",
        "**Essential Syntaxes**\n",
        "\n",
        "python\n",
        "set_seed(42)\n",
        "\n",
        "    Fixes random seeds for all libraries, ensuring results are reproducible across different runs.\n",
        "\n",
        "python\n",
        "trainer.train()\n",
        "\n",
        "    Trains your BERT QA model on the training data using the best hyperparameters from your random search.\n",
        "\n",
        "python\n",
        "trainer.evaluate()\n",
        "\n",
        "    Evaluates the trained model's performance on your held-out validation dataset.\n",
        "\n",
        "python\n",
        "trainer.save_model(\"/content/drive/My Drive/bert_qa_best_random_hps\")\n",
        "\n",
        "    Saves the complete, fine-tuned model checkpoint to Google Drive so you can load it later for question-answering tasks.\n",
        "\n",
        "**Example Output**\n",
        "\n",
        "\n",
        "--- Starting Fine-Tuning with Best Hyperparameters (Expected Time: 1â€“4 hours on GPU) ---\n",
        "\n",
        "[284/284 02:38, Epoch 4/4]\n",
        "Epoch \tTraining Loss \tValidation Loss \tExact Match \tF1 Score \tAvg Inference Time\n",
        "1 \tNo log \t1.811164 \t0.000000 \t0.034671 \t0.000789\n",
        "2 \t1.607500 \t1.942523 \t0.000000 \t0.235041 \t0.001137\n",
        "3 \t1.159300 \t2.277214 \t0.000000 \t0.270486 \t0.002345\n",
        "4 \t1.159300 \t2.429585 \t0.000000 \t0.294740 \t0.000993\n",
        "\n",
        "**Comment and Observation**\n",
        "\n",
        "This phase puts your best-found hyperparameters into practice, ensuring the training results you see are reliable and repeatable by setting the seed."
      ],
      "metadata": {
        "id": "Dnh8h45Rrmp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "print(\"\\n--- Starting Fine-Tuning with Best Hyperparameters (Expected Time: 1â€“4 hours on GPU) ---\")\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n--- Final Evaluation Results with Best Hyperparameters ---\")\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n",
        "\n",
        "trainer.save_model(\"/content/drive/My Drive/bert_qa_best_random_hps\")\n",
        "\n",
        "print(\"\\n Fine-tuning process complete with best hyperparameters. The resulting model can now be used for Inference (Stage 4).\")"
      ],
      "metadata": {
        "id": "rg_mh4I3Qrq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bWZzEP3NAW0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Execution using the best hyperparameters found (Grid Search)**"
      ],
      "metadata": {
        "id": "CUL-24JbDKgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function Description**\n",
        "\n",
        "This code configures and initializes a Hugging Face Trainer using the best hyperparameters obtained from a prior grid search for fine-tuning a BERT question answering model. It sets up training arguments such as learning rate, batch size, number of epochs, and device preferences, then prepares the trainer with the selected datasets, metrics, and tokenizer for the final training run.\n",
        "\n",
        "**Input**\n",
        "\n",
        "    Best hyperparameters dictionary (best_hps) containing keys: num_train_epochs, per_device_train_batch_size, and learning_rate discovered from grid search.\n",
        "\n",
        "    Pretrained BERT model instance (model).\n",
        "\n",
        "    Tokenized training and evaluation datasets (tokenized_train and tokenized_eval).\n",
        "\n",
        "    Predefined evaluation metric function (compute_metrics).\n",
        "\n",
        "    Tokenizer for data collation.\n",
        "\n",
        "    Default data collator to handle batch preparation.\n",
        "\n",
        "**Output**\n",
        "\n",
        "    An initialized Trainer object configured to train the model using the best hyperparameters.\n",
        "\n",
        "    Printed confirmation output displaying the current training arguments, including directories, learning rate, batch size, and other settings.\n",
        "\n",
        "**Essential Syntaxes**\n",
        "\n",
        "python\n",
        "best_training_args = TrainingArguments(\n",
        "    output_dir=...,\n",
        "    num_train_epochs=best_hps['num_train_epochs'],\n",
        "    per_device_train_batch_size=best_hps['per_device_train_batch_size'],\n",
        "    learning_rate=best_hps['learning_rate'],\n",
        "    ...\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    seed=42,\n",
        "    data_seed=42,\n",
        ")\n",
        "\n",
        "    Creates training argument instance using the best hyperparameters while maintaining consistent logging and saving configurations.\n",
        "\n",
        "python\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=best_training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "    Sets up the Hugging Face Trainer with the finalized arguments, enabling efficient training and evaluation.\n",
        "\n",
        "python\n",
        "print(best_training_args)\n",
        "\n",
        "    Prints the training configuration to confirm settings prior to starting training.\n",
        "\n",
        "**Example Output**\n",
        "\n",
        "\n",
        "TrainingArguments(\n",
        "  output_dir=/content/drive/My Drive/results_best_hps,\n",
        "  num_train_epochs=5,\n",
        "  per_device_train_batch_size=8,\n",
        "  per_device_eval_batch_size=8,\n",
        "  warmup_steps=500,\n",
        "  weight_decay=0.01,\n",
        "  learning_rate=3e-05,\n",
        "  logging_dir=/content/drive/My Drive/logs_best_hps,\n",
        "  save_strategy=epoch,\n",
        "  eval_strategy=epoch,\n",
        "  load_best_model_at_end=True,\n",
        "  fp16=True,\n",
        "  seed=42,\n",
        "  data_seed=42,\n",
        ")\n",
        "\n",
        "**Comment and Observation**\n",
        "\n",
        "This code completes the model fine-tuning pipeline by initializing the Trainer with carefully selected hyperparameters validated through grid search. Using load_best_model_at_end=True ensures that the best checkpoint saved during training will be loaded for final evaluations or deployment."
      ],
      "metadata": {
        "id": "1XX5jCmxoTe6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad4d048f"
      },
      "source": [
        "\n",
        "best_training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/My Drive/results_best_hps\",\n",
        "    num_train_epochs=best_hps['num_train_epochs'],\n",
        "    per_device_train_batch_size=best_hps['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=best_hps['per_device_train_batch_size'],\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"/content/drive/My Drive/logs_best_hps\",\n",
        "    learning_rate=best_hps['learning_rate'],\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=[],\n",
        "    seed=42,\n",
        "    data_seed=42\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=best_training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "print(\"\\n--- Initialized Trainer with Best Hyperparameters ---\")\n",
        "print(best_training_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function Description\n",
        "\n",
        "This code block performs the final stages of BERT-based question answering model fine-tuning, including:\n",
        "\n",
        "    Training the model using previously identified best hyperparameters.\n",
        "\n",
        "    Evaluating the fine-tuned model's performance on the evaluation dataset.\n",
        "\n",
        "    Saving the trained model checkpoint for future inference.\n",
        "\n",
        "    Ensuring all random seeds are set for reproducibility.\n",
        "\n",
        "Input\n",
        "\n",
        "    The trainer object: configured with your best hyperparameters, model, datasets, tokenizer, and metric function.\n",
        "\n",
        "    Google Drive path for saving the model checkpoint (as a string).\n",
        "\n",
        "Output\n",
        "\n",
        "    Console messages indicating the progress of training and evaluation.\n",
        "\n",
        "    Printed final evaluation metrics (like F1 Score, Exact Match).\n",
        "\n",
        "    The trained model saved in your specified Google Drive directory for Stage 4 (Inference).\n",
        "\n",
        "Essential Syntaxes\n",
        "\n",
        "python\n",
        "set_seed(42)\n",
        "\n",
        "    Fixes all Python and GPU-related random sources to make training and evaluation reproducible.\n",
        "\n",
        "python\n",
        "trainer.train()\n",
        "\n",
        "    Triggers model fine-tuning (training) using the best hyperparameters found during search.\n",
        "\n",
        "python\n",
        "trainer.evaluate()\n",
        "\n",
        "    Runs evaluation on the held-out dataset to report final performance metrics.\n",
        "\n",
        "python\n",
        "trainer.save_model(\"/content/drive/My Drive/bert_qa_best_hps\")\n",
        "\n",
        "    Saves a complete checkpoint of your trained model (weights, config, and tokenizer) to Google Drive. You can reload this checkpoint later for QA inference.\n",
        "\n",
        "Example Output\n",
        "\n",
        "text\n",
        "--- Starting Fine-Tuning with Best Hyperparameters (Expected Time: 1â€“4 hours on GPU) ---\n",
        "\n",
        "[Training progress output]\n",
        "\n",
        "--- Final Evaluation Results with Best Hyperparameters ---\n",
        "{'eval_loss': 1.02, 'eval_Exact_Match': 0.76, 'eval_F1_Score': 0.81, ...}\n",
        "\n",
        "Fine-tuning process complete with best hyperparameters. The resulting model can now be used for Inference (Stage 4).\n",
        "\n",
        "Comment and Observation\n",
        "\n",
        "This final phase puts everything together: your model is trained on the best settings, thoroughly evaluated, and safely saved for later use."
      ],
      "metadata": {
        "id": "lDvFu1X_qODE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "826bf16c"
      },
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "\n",
        "print(\"\\n--- Starting Fine-Tuning with Best Hyperparameters (Expected Time: 1â€“4 hours on GPU) ---\")\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "print(\"\\n--- Final Evaluation Results with Best Hyperparameters ---\")\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n",
        "\n",
        "trainer.save_model(\"/content/drive/My Drive/bert_qa_best_hps\")\n",
        "\n",
        "print(\"\\n Fine-tuning process complete with best hyperparameters. The resulting model can now be used for Inference (Stage 4).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Execution using the best hyperparameters found (Random Search)**"
      ],
      "metadata": {
        "id": "Xpl6mXprDVDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function Description**\n",
        "\n",
        "This code block sets up a Hugging Face Trainer to fine-tune a BERT-based question answering model using the best hyperparameters found from a random search. It extracts the optimal combination of hyperparameters from your trial results and configures the training process to use these settings for a final training run.\n",
        "Input\n",
        "\n",
        "    best_hps_random_search: A dictionary containing the best hyperparameters (epochs, batch_size, learning_rate) identified during random search.\n",
        "\n",
        "    Pretrained model and tokenizer for BERT QA.\n",
        "\n",
        "    Tokenized training and evaluation datasets (tokenized_train, tokenized_eval).\n",
        "\n",
        "    Previously defined metric function (compute_metrics).\n",
        "\n",
        "**Output**\n",
        "\n",
        "    An initialized Trainer object configured with the selected best hyperparametersâ€”ready for fine-tuning and evaluation.\n",
        "\n",
        "    Printed confirmation of the training arguments for transparency and tracking.\n",
        "\n",
        "**Essential Syntaxes**\n",
        "\n",
        "python\n",
        "best_hps_random_search = df_results.iloc[0].to_dict()\n",
        "\n",
        "    Grabs the best hyperparameter set as a Python dictionary from your sorted trial results DataFrame.\n",
        "\n",
        "python\n",
        "best_training_args_random_search = TrainingArguments(\n",
        "    output_dir=...,  # folder for saving model outputs\n",
        "    num_train_epochs=int(best_hps_random_search['epochs']),\n",
        "    per_device_train_batch_size=int(best_hps_random_search['batch_size']),\n",
        "    learning_rate=best_hps_random_search['learning_rate'],\n",
        "    ...\n",
        ")\n",
        "\n",
        "    Initializes TrainingArguments with the values from your best random search trial.\n",
        "\n",
        "python\n",
        "trainer_random_search = Trainer(\n",
        "    model=BertForQuestionAnswering.from_pretrained(MODEL_NAME).to(device),\n",
        "    args=best_training_args_random_search,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "    Prepares a new Trainer object with all required settings for final training.\n",
        "\n",
        "**Example Output**\n",
        "\n",
        "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "\n",
        "\n",
        "--- Initialized Trainer with Best Hyperparameters from Random Search ---\n",
        "TrainingArguments(\n",
        "_n_gpu=1,\n",
        "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
        "adafactor=False,\n",
        "adam_beta1=0.9,\n",
        "adam_beta2=0.999,\n",
        "adam_epsilon=1e-08,\n",
        "auto_find_batch_size=False,\n",
        "average_tokens_across_devices=True,\n",
        "batch_eval_metrics=False,\n",
        "bf16=False,\n",
        "bf16_full_eval=False,\n",
        "data_seed=42,\n",
        "\n",
        "**Comment and Observation**\n",
        "\n",
        "This block ensures you are leveraging the most effective hyperparameter configuration discovered during random search. By saving outputs and logs to Google Drive, results and checkpoints are preserved for later validation or deployment"
      ],
      "metadata": {
        "id": "fNRfTg_wqk0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_hps_random_search = df_results.iloc[0].to_dict()\n",
        "\n",
        "\n",
        "best_training_args_random_search = TrainingArguments(\n",
        "    output_dir=\"/content/drive/My Drive/results_best_hps_random_search\",\n",
        "    num_train_epochs=int(best_hps_random_search['epochs']),\n",
        "    per_device_train_batch_size=int(best_hps_random_search['batch_size']),\n",
        "    per_device_eval_batch_size=int(best_hps_random_search['batch_size']),\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"/content/drive/My Drive/logs_best_hps_random_search\",\n",
        "    learning_rate=best_hps_random_search['learning_rate'],\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=[],\n",
        "    seed=42,\n",
        "    data_seed=42\n",
        ")\n",
        "\n",
        "\n",
        "trainer_random_search = Trainer(\n",
        "    model=BertForQuestionAnswering.from_pretrained(MODEL_NAME).to(device),\n",
        "    args=best_training_args_random_search,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "print(\"\\n--- Initialized Trainer with Best Hyperparameters from Random Search ---\")\n",
        "print(best_training_args_random_search)"
      ],
      "metadata": {
        "id": "DPvK-UZmCwun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function Description\n",
        "\n",
        "This code performs the final round of fine-tuning for your BERT-based QA model using the best hyperparameters discovered from random search. It takes care to set seeds for reproducibility, starts model training, evaluates your model, saves the best checkpoint, and prints friendly, readable progress updates throughout.\n",
        "Input\n",
        "\n",
        "    The trainer_random_search object: a Hugging Face Trainer, pre-loaded with the model, datasets, tokenizer, metrics, and best random search hyperparameters.\n",
        "\n",
        "    Save path for the trained model checkpoint (here: /content/drive/My Drive/bert_qa_best_random_hps).\n",
        "\n",
        "Output\n",
        "\n",
        "    Console logs and printed messages showing the start, completion, and high-level metrics of fine-tuning and evaluation.\n",
        "\n",
        "    Final evaluation dictionary (eval_results_random_search) summarizing performance (e.g., F1 Score, Exact Match).\n",
        "\n",
        "    Saved model checkpoint for future inference.\n",
        "\n",
        "Essential Syntaxes\n",
        "\n",
        "python\n",
        "set_seed(42)\n",
        "\n",
        "Sets all sources of randomness in your environment. This is crucial for reliable, repeatable experiments.\n",
        "\n",
        "python\n",
        "trainer_random_search.train()\n",
        "\n",
        "Begins fine-tuning using best random search settings. This usually takes 1â€“4 hours with a GPU â€” so sit back, maybe grab a coffee while it runs!\n",
        "\n",
        "â€‹\n",
        "\n",
        "python\n",
        "trainer_random_search.evaluate()\n",
        "\n",
        "Checks the model against your validation set and reports metrics, helping you measure how successful fine-tuning was.\n",
        "\n",
        "python\n",
        "trainer_random_search.save_model(...)\n",
        "\n",
        "Stores your fully trained model on Google Drive, so you donâ€™t need to retrain later.\n",
        "Example Output\n",
        "\n",
        "text\n",
        "--- Starting Fine-Tuning with Best Hyperparameters from Random Search (Expected Time: 1â€“4 hours on GPU) ---\n",
        "\n",
        "[Training progress output]\n",
        "\n",
        "--- Final Evaluation Results with Best Hyperparameters from Random Search ---\n",
        "{'eval_loss': 0.98, 'eval_Exact_Match': 0.80, 'eval_F1_Score': 0.87, ...}\n",
        "\n",
        "Fine-tuning process complete with best hyperparameters from random search. The resulting model can now be used for Inference (Stage 4).\n",
        "\n",
        "**Comment and Observation**\n",
        "\n",
        "This workflow is designed to be user-friendly and transparent: you get clear status updates before, during, and after training. This is just a test Dr. Raga but as you can see this is the flow that we want to build which is q & a essay type."
      ],
      "metadata": {
        "id": "tvehdkEjKFR9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4afe6f6c"
      },
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "print(\"\\n--- Starting Fine-Tuning with Best Hyperparameters from Random Search (Expected Time: 1â€“4 hours on GPU) ---\")\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "trainer_random_search.train()\n",
        "\n",
        "print(\"\\n--- Final Evaluation Results with Best Hyperparameters from Random Search ---\")\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "eval_results_random_search = trainer_random_search.evaluate()\n",
        "print(eval_results_random_search)\n",
        "\n",
        "trainer_random_search.save_model(\"/content/drive/My Drive/bert_qa_best_random_hps\")\n",
        "\n",
        "print(\"\\n Fine-tuning process complete with best hyperparameters from random search. The resulting model can now be used for Inference (Stage 4).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Execution using the all best hyperparameters found (Random Search & Grid Search)**"
      ],
      "metadata": {
        "id": "YRPf8q0LAqYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_hps = df_results.iloc[0].to_dict() # Best hyperparameters that I found (num_train_epochs, per_device_train_batch_size, per_device_eval_batch_size, learning_rate)\n",
        "\n",
        "\n",
        "best_training_args_with_best_hps = TrainingArguments(\n",
        "    output_dir=\"/content/drive/My Drive/results_best_hps_random_search\",\n",
        "    num_train_epochs=int(best_hps['epochs']),\n",
        "    per_device_train_batch_size=int(best_hps['batch_size']),\n",
        "    per_device_eval_batch_size=int(best_hps['batch_size']),\n",
        "    warmup_steps=500,   # Best warmup_steps found from my groupmates\n",
        "    weight_decay=0.75, # Best weight decay found from my groupmates\n",
        "    logging_dir=\"/content/drive/My Drive/logs_best_hps_random_search\",\n",
        "    learning_rate=best_hps['learning_rate'],\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\", # Best eval_strategy found from my groupmates\n",
        "    save_strategy=\"epoch\", # Best save_strategy found from my groupmates\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=[],\n",
        "    seed=42,\n",
        "    data_seed=42\n",
        ")\n",
        "\n",
        "\n",
        "trainer_with_best_hps = Trainer(\n",
        "    model=BertForQuestionAnswering.from_pretrained(MODEL_NAME).to(device),\n",
        "    args=best_training_args_with_best_hps,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer_with_best_hps.train()\n",
        "\n",
        "print(\"\\n--- Trainer with Best Hyperparameters ---\")\n",
        "print(best_training_args_with_best_hps)\n",
        "\n",
        "best_model_path = \"/content/drive/My Drive/best_qa_model\"\n",
        "trainer_with_best_hps.save_model(best_model_path)\n",
        "tokenizer.save_pretrained(best_model_path)\n",
        "print(f\"\\nModel saved to {best_model_path}\")"
      ],
      "metadata": {
        "id": "4TH_a3GvAsaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Actual Testing**"
      ],
      "metadata": {
        "id": "QWTDiCCgJZVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function Description**\n",
        "\n",
        "This code creates an interactive question-answering session using a Hugging Face model pipeline. For each question from your DataFrame, it poses the question, gets your answer, compares it to the ground truth using exact match and F1 metrics, and also measures the semantic similarity using embeddings and cosine similarity. It uses the Hugging Face pipeline utility for simple and effective inference.\n",
        "\n",
        "**Input**\n",
        "\n",
        "    df: A DataFrame containing columns \"Question\", \"Context\", and \"Answer\" for each example.\n",
        "\n",
        "    User input (your answer to each posed question).\n",
        "\n",
        "**Output**\n",
        "\n",
        "    Prints detailed evaluation for each test round: ground truth, model's predicted answer, your answer, Exact Match, F1 score, cosine similarity, and inference time.\n",
        "\n",
        "**Essential Syntaxes**\n",
        "\n",
        "python\n",
        "qna_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device_num\n",
        ")\n",
        "\n",
        "    Sets up a question-answering pipeline using your fine-tuned model and tokenizer. Uses GPU if available, else CPU.\n",
        "\n",
        "python\n",
        "def get_embedding(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.bert(**inputs)\n",
        "    return outputs.last_hidden_state[0][0].cpu().numpy()\n",
        "\n",
        "    Computes the embedding for a given text by passing it through the base BERT model. Used to compare semantic similarity of answers.\n",
        "\n",
        "python\n",
        "bert_result = qna_pipeline({\"question\": row['Question'], \"context\": row['Context']})\n",
        "\n",
        "    Performs inference: predicts the answer span using the model.\n",
        "\n",
        "python\n",
        "exact = compute_exact_match(user_answer, row['Answer'])\n",
        "f1 = compute_f1(user_answer, row['Answer'])\n",
        "\n",
        "    Calculates the exact match and F1 metrics for your answer against the reference answer.\n",
        "\n",
        "python\n",
        "cos_sim = cosine_similarity([user_emb], [gt_emb])[0][0]\n",
        "\n",
        "    Finds cosine similarity (semantic closeness) between your answer and the ground truth, using embeddings.\n",
        "\n",
        "**Example Output**\n",
        "\n",
        "text\n",
        "Question: 1\n",
        "What does GeeksforGeeks provide?\n",
        "Please type your answer:\n",
        "<user types>\n",
        "\n",
        "Evaluation\n",
        "Ground Truth Answer: resources for computer science\n",
        "Model Predicted Answer: resources for computer science\n",
        "Your Answer: resources for computer science\n",
        "Exact Match: 1\n",
        "F1 Score: 1.0000\n",
        "Cosine Similarity: 1.0000\n",
        "Inference Time (BERT QA): 0.1122 seconds\n",
        "\n",
        "**Comment and Observation**\n",
        "\n",
        "This approach is ideal for human-in-the-loop evaluation. You can check your own understanding, see how the model performs, and compare results both exactly (Exact Match, F1) and by semantics (cosine similarity of embeddings)."
      ],
      "metadata": {
        "id": "7yr1pifb8rGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# import time\n",
        "# import torch\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from transformers import pipeline, BertForQuestionAnswering, BertTokenizer\n",
        "# import re # Import regex for placeholder F1 score calculation\n",
        "# from collections import Counter # Import Counter for F1 score calculation\n",
        "\n",
        "\n",
        "# # Load the fine-tuned model and tokenizer\n",
        "\n",
        "# best_model_path  = \"/content/drive/My Drive/best_qa_model\"\n",
        "# model = BertForQuestionAnswering.from_pretrained(best_model_path)\n",
        "# tokenizer = BertTokenizer.from_pretrained(best_model_path)\n",
        "\n",
        "\n",
        "# device_num = 0 if torch.cuda.is_available() else -1\n",
        "# device = torch.device(f\"cuda:{device_num}\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device) # Move the model to the correct device\n",
        "\n",
        "\n",
        "# correct_answer_treshold = 0.75 # Cosine similarity score to be considered a \"correct\" answer\n",
        "# mastery_threshold = 0.80 # Percentage of correct answers (e.g., 9 out of 10) to master a topic\n",
        "# max_questions_topic = 10 # Max Questions Per Topic\n",
        "\n",
        "\n",
        "# qna_pipeline = pipeline(\n",
        "#     \"question-answering\",\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     device=device_num\n",
        "# )\n",
        "\n",
        "# def get_embedding(text, tokenizer, model):\n",
        "#     # Ensure model is in evaluation mode\n",
        "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.bert(**inputs)\n",
        "#     return outputs.last_hidden_state[0][0].cpu().numpy()\n",
        "\n",
        "\n",
        "# def choos_QnA_mode(df):\n",
        "\n",
        "#    available_modes = ['Quiz', 'Exam']\n",
        "\n",
        "#    print(\"\\nAvailable Modes:\")\n",
        "#    for m in available_modes:\n",
        "#        print(f\"- {m}\")\n",
        "\n",
        "#    while True:\n",
        "#        user_input = input(\"\\nType the mode you want to study: \").strip().lower()\n",
        "\n",
        "#        if user_input in available_modes:\n",
        "#             print(f\"\\nYou selected QnA mode: {user_input}\")\n",
        "#             return user_input\n",
        "#         else:\n",
        "#             print(\"Invalid mode. Please choose from the available options.\")\n",
        "\n",
        "\n",
        "\n",
        "# def choose_topic(df):\n",
        "#     available_topics = df['Title'].unique().tolist()\n",
        "\n",
        "#     print(\"\\nAvailable Topics:\")\n",
        "#     for t in available_topics:\n",
        "#         print(f\"- {t}\")\n",
        "\n",
        "#     while True:\n",
        "#         user_input = input(\"\\nType the topic you want to study: \").strip().lower()\n",
        "\n",
        "#         # Exact match\n",
        "#         for topic in available_topics:\n",
        "#             if user_input == topic.lower():\n",
        "#                 print(f\"\\nYou selected topic: {topic}\")\n",
        "#                 return topic\n",
        "\n",
        "#         # Partial match\n",
        "#         partial_matches = [topic for topic in available_topics if user_input in topic.lower()]\n",
        "\n",
        "#         if len(partial_matches) == 1:\n",
        "#             print(f\"\\nYou selected topic: {partial_matches[0]}\")\n",
        "#             return partial_matches[0]\n",
        "\n",
        "#         elif len(partial_matches) > 1:\n",
        "#             print(\"\\nDid you mean one of these?\")\n",
        "#             for pm in partial_matches:\n",
        "#                 print(f\"- {pm}\")\n",
        "#             print(\"Please type the full topic name.\")\n",
        "\n",
        "#         else:\n",
        "#             print(\"\\nTopic not found. Please try again.\")\n",
        "#             print(\"Available topics include:\")\n",
        "#             for t in available_topics:\n",
        "#                 print(f\"- {t}\")\n",
        "\n",
        "\n",
        "\n",
        "# def interactive_question_answering(df, topic, asked_indices_set, question_answering_times, qna_mode):\n",
        "#     \"\"\"\n",
        "#     df: full dataframe\n",
        "#     topic: chosen Title (string)\n",
        "#     asked_indices_set: a set() of df.index values already asked for this session,\n",
        "#                        used to avoid repeats.\n",
        "#     Returns: the index asked (or None if no more questions)\n",
        "#     \"\"\"\n",
        "\n",
        "#     if(qna_mode == 'Quiz'):\n",
        "#         question_mode_column = 'Quiz Question'\n",
        "#     elif qna_mode == 'Exam':\n",
        "#         question_mode_column = 'Exam Question'\n",
        "#     else:\n",
        "#         print(\"Invalid QnA Mode. Must be 'Quiz' or 'Exam'.\")\n",
        "#         return None, False\n",
        "\n",
        "#     # Filter by topic\n",
        "#     topic_df = df[df['Title'] == topic].copy()\n",
        "\n",
        "#     # Filter out rrows that don't have a valid question for the selected mode\n",
        "#     topic_df.dropna(subset=[mode_column], inplace=True)\n",
        "\n",
        "#     # Filter out rows that have already been asked\n",
        "#     topic_df = topic_df[~topic_df.index.isin(asked_indices_set)]\n",
        "\n",
        "#     if topic_df.empty:\n",
        "#         print(\"\\nNo questions available for this topic.\")\n",
        "#         return None\n",
        "\n",
        "\n",
        "#     filtered = topic_df\n",
        "\n",
        "#     if filtered.empty:\n",
        "#         print(\"\\nNo essay-type questions available for this topic (or after filtering).\")\n",
        "#         return None\n",
        "\n",
        "#     # Get remaining rows (not asked yet)\n",
        "#     remaining = filtered[~filtered.index.isin(asked_indices_set)]\n",
        "\n",
        "#     if remaining.empty:\n",
        "#         # If no unseen questions left, either reset or tell user\n",
        "#         print(\"\\nYou've seen all available questions for this topic.\")\n",
        "#         return None\n",
        "\n",
        "#     # Pick one random row (without replacement)\n",
        "#     row = remaining.sample(n=1).iloc[0]\n",
        "#     row_idx = row.name  # dataframe index\n",
        "\n",
        "#     # Use the correct columns for question and anser\n",
        "#     question_text = row[question_mode_column]\n",
        "#     ground_truth_answer = row[question_mode_column]\n",
        "\n",
        "#     # Show question and collect answer\n",
        "#     print(f\"\\nQuestion {question_answering_times}:\")\n",
        "#     print(row['Question'])\n",
        "\n",
        "#     user_answer = input(\"\\nPlease type your answer:\\n\")\n",
        "\n",
        "#     # Evaluate with your existing functions / model\n",
        "#     start = time.time()\n",
        "#     bert_result = qna_pipeline({\n",
        "#         \"question\": row['Question'],\n",
        "#         \"context\": row.get('Context', '')\n",
        "#     })\n",
        "#     inference_time = time.time() - start\n",
        "\n",
        "#     exact = compute_exact_match(user_answer, row.get('Answer', ''))\n",
        "#     f1 = compute_f1(user_answer, row.get('Answer', ''))\n",
        "\n",
        "#     user_emb = get_embedding(user_answer, tokenizer, model)\n",
        "#     gt_emb = get_embedding(row.get('Answer', ''), tokenizer, model)\n",
        "\n",
        "#     cos_sim = cosine_similarity([user_emb], [gt_emb])[0][0]\n",
        "\n",
        "#     print(\"\\n Evaluation Per Question\")\n",
        "#     print(f\"Ground Truth Answer: {row.get('Answer', '')}\")\n",
        "#     print(f\"Model Predicted Answer: {bert_result.get('answer', '')}\")\n",
        "#     print(f\"Your Answer: {user_answer}\")\n",
        "#     print(f\"Exact Match: {exact}\")\n",
        "#     print(f\"F1 Score: {f1:.4f}\")\n",
        "#     print(f\"Cosine Similarity: {cos_sim:.4f}\")\n",
        "\n",
        "#     is_correct = cos_sim >= correct_answer_treshold\n",
        "#     if is_correct:\n",
        "#         print(\"Result: Correct! Your answer is semantically similar to the expected one.\")\n",
        "#     else:\n",
        "#         print(\"Result: Incorrect. Need Improvement. Your answer is a bit different from the expected one.\")\n",
        "#     print(f\"Inference Time: {inference_time:.4f} seconds\\n\")\n",
        "\n",
        "#     return row_idx, is_correct\n",
        "\n",
        "\n",
        "\n",
        "# def study_session(df):\n",
        "\n",
        "#   question_times = 0 # Number of Questions\n",
        "\n",
        "#   while True: # Loop for the entire session, continues until user quits\n",
        "#     topic = choose_topic(df)\n",
        "\n",
        "#     # if topic is none: # User typed 'quit'\n",
        "#     #   break\n",
        "\n",
        "\n",
        "#     # Start of a new topic round\n",
        "#     asked_indices_this_topic = set()\n",
        "#     correct_answers_this_topic = 0\n",
        "#     questions_asked_this_topic = 0\n",
        "\n",
        "#     for i in range (max_questions_topic):\n",
        "#         question_times += 1\n",
        "\n",
        "#         idx, is_correct = interactive_question_answering(df, topic, asked_indices_this_topic, question_answering_times=question_times)\n",
        "#         if idx is None:\n",
        "#             break\n",
        "\n",
        "\n",
        "#         asked_indices_this_topic.add(idx)\n",
        "#         questions_asked_this_topic += 1\n",
        "\n",
        "#         if is_correct:\n",
        "#            correct_answers_this_topic += 1\n",
        "\n",
        "\n",
        "#          # Topic Round-up and Mastery Check\n",
        "#         if questions_asked_this_topic != 0:\n",
        "#             score_percentage = correct_answers_this_topic / questions_asked_this_topic\n",
        "#             print(f\"\\nTopic Summary for '{topic}'\")\n",
        "#             print(f\"You answered {correct_answers_this_topic} out of {questions_asked_this_topic} questions correctly.\")\n",
        "\n",
        "#             if score_percentage >= mastery_threshold:\n",
        "#                 print(\"Congratulations! You have a good grasp of this topic.\")\n",
        "#             else:\n",
        "#                 print(\"You're making progress! A little more practice on this topic would be helpful.\")\n",
        "\n",
        "#     print(\"\\nStudy session finished. Great work!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# study_session(df)\n",
        "\n",
        "\n",
        "# # print(\"\\nStudy session finished!\")\n",
        "\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline, BertForQuestionAnswering, BertTokenizer\n",
        "import re # Import regex for placeholder F1 score calculation\n",
        "from collections import Counter # Import Counter for F1 score calculation\n",
        "\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "\n",
        "best_model_path  = \"/content/drive/My Drive/best_qa_model\"\n",
        "model = BertForQuestionAnswering.from_pretrained(best_model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(best_model_path)\n",
        "\n",
        "\n",
        "device_num = 0 if torch.cuda.is_available() else -1\n",
        "device = torch.device(f\"cuda:{device_num}\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device) # Move the model to the correct device\n",
        "\n",
        "\n",
        "correct_answer_treshold = 0.75 # Cosine similarity score to be considered a \"correct\" answer\n",
        "mastery_threshold = 0.80 # Percentage of correct answers (e.g., 9 out of 10) to master a topic\n",
        "max_questions_topic = 10 # Max Questions Per Topic\n",
        "\n",
        "\n",
        "qna_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device_num\n",
        ")\n",
        "\n",
        "def get_embedding(text, tokenizer, model):\n",
        "    # Ensure model is in evaluation mode\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.bert(**inputs)\n",
        "    return outputs.last_hidden_state[0][0].cpu().numpy()\n",
        "\n",
        "\n",
        "def choose_QnA_mode(): # Corrected function name\n",
        "   available_modes = ['Quiz', 'Exam']\n",
        "\n",
        "   print(\"\\nAvailable Modes:\")\n",
        "   for m in available_modes:\n",
        "       print(f\"- {m}\")\n",
        "\n",
        "   while True:\n",
        "       user_input = input(\"\\nType the mode you want to study: \").strip().title() # Use .title() for consistent capitalization\n",
        "\n",
        "       if user_input in available_modes:\n",
        "            print(f\"\\nYou selected QnA mode: {user_input}\")\n",
        "            return user_input\n",
        "       else:\n",
        "            print(\"Invalid mode. Please choose from 'Quiz' or 'Exam'.\")\n",
        "\n",
        "\n",
        "def choose_topic(df):\n",
        "    available_topics = df['Title'].unique().tolist()\n",
        "\n",
        "    print(\"\\nAvailable Topics:\")\n",
        "    for t in available_topics:\n",
        "        print(f\"- {t}\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nType the topic you want to study: \").strip().lower()\n",
        "\n",
        "        # Exact match\n",
        "        for topic in available_topics:\n",
        "            if user_input == topic.lower():\n",
        "                print(f\"\\nYou selected topic: {topic}\")\n",
        "                return topic\n",
        "\n",
        "        # Partial match\n",
        "        partial_matches = [topic for topic in available_topics if user_input in topic.lower()]\n",
        "\n",
        "        if len(partial_matches) == 1:\n",
        "            print(f\"\\nYou selected topic: {partial_matches[0]}\")\n",
        "            return partial_matches[0]\n",
        "\n",
        "        elif len(partial_matches) > 1:\n",
        "            print(\"\\nDid you mean one of these?\")\n",
        "            for pm in partial_matches:\n",
        "                print(f\"- {pm}\")\n",
        "            print(\"Please type the full topic name.\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\nTopic not found. Please try again.\")\n",
        "            print(\"Available topics include:\")\n",
        "            for t in available_topics:\n",
        "                print(f\"- {t}\")\n",
        "\n",
        "\n",
        "def interactive_question_answering(df, topic, asked_indices_set, question_answering_times, qna_mode):\n",
        "    \"\"\"\n",
        "    df: full dataframe\n",
        "    topic: chosen Title (string)\n",
        "    asked_indices_set: a set() of df.index values already asked for this session,\n",
        "                       used to avoid repeats.\n",
        "    qna_mode: 'Quiz' or 'Exam'\n",
        "    Returns: the index asked (or None) and whether the answer was correct\n",
        "    \"\"\"\n",
        "    # MODIFIED: Determine the correct question and answer columns based on the mode\n",
        "    if qna_mode == 'Quiz':\n",
        "        question_col = 'Quiz Question'\n",
        "        answer_col = 'Quiz Answer'\n",
        "    elif qna_mode == 'Exam':\n",
        "        question_col = 'Exam Question'\n",
        "        answer_col = 'Exam Answer'\n",
        "    else:\n",
        "        print(\"Invalid QnA Mode passed to function.\")\n",
        "        return None, False\n",
        "\n",
        "    # Filter by topic\n",
        "    topic_df = df[df['Title'] == topic].copy()\n",
        "\n",
        "    # Filter out rows that don't have a valid question for the selected mode\n",
        "    topic_df.dropna(subset=[question_col], inplace=True)\n",
        "    topic_df = topic_df[topic_df[question_col].str.strip() != '']\n",
        "\n",
        "    if topic_df.empty:\n",
        "        print(f\"\\nNo '{qna_mode}' questions are available for this topic.\")\n",
        "        return None, False\n",
        "\n",
        "    # Get remaining rows (not asked yet)\n",
        "    remaining = topic_df[~topic_df.index.isin(asked_indices_set)]\n",
        "\n",
        "    if remaining.empty:\n",
        "        print(\"\\nYou've seen all available questions for this topic in this mode.\")\n",
        "        return None, False\n",
        "\n",
        "    # Pick one random row (without replacement)\n",
        "    row = remaining.sample(n=1).iloc[0]\n",
        "    row_idx = row.name  # dataframe index\n",
        "\n",
        "    # MODIFIED: Use the correct columns for question and answer\n",
        "    question_text = row[question_col]\n",
        "    ground_truth_answer = row[answer_col]\n",
        "\n",
        "    # Show question and collect answer\n",
        "    print(f\"\\n--- Question {question_answering_times} ---\")\n",
        "    print(question_text)\n",
        "\n",
        "    user_answer = input(\"\\nPlease type your answer:\\n\")\n",
        "\n",
        "    # Evaluate with your existing functions / model\n",
        "    start = time.time()\n",
        "    bert_result = qna_pipeline({\n",
        "        \"question\": question_text,\n",
        "        \"context\": row.get('Context', '')\n",
        "    })\n",
        "    inference_time = time.time() - start\n",
        "\n",
        "    # MODIFIED: Compare against the correct ground truth answer\n",
        "    exact = compute_exact_match(user_answer, ground_truth_answer)\n",
        "    f1 = compute_f1(user_answer, ground_truth_answer)\n",
        "\n",
        "    user_emb = get_embedding(user_answer, tokenizer, model)\n",
        "    gt_emb = get_embedding(ground_truth_answer, tokenizer, model)\n",
        "\n",
        "    cos_sim = cosine_similarity([user_emb], [gt_emb])[0][0]\n",
        "\n",
        "    print(\"\\n--- Evaluation Per Question ---\")\n",
        "    print(f\"Ground Truth Answer: {ground_truth_answer}\")\n",
        "    print(f\"Model Predicted Answer: {bert_result.get('answer', '')}\")\n",
        "    print(f\"Your Answer: {user_answer}\")\n",
        "    print(f\"Exact Match: {exact}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Cosine Similarity: {cos_sim:.4f}\")\n",
        "\n",
        "    is_correct = cos_sim >= correct_answer_treshold\n",
        "    if is_correct:\n",
        "        print(\"Result: Correct! Your answer is semantically similar to the expected one.\")\n",
        "    else:\n",
        "        print(\"Result: Incorrect. Need Improvement. Your answer is a bit different from the expected one.\")\n",
        "    print(f\"Inference Time: {inference_time:.4f} seconds\\n\")\n",
        "\n",
        "    return row_idx, is_correct\n",
        "\n",
        "\n",
        "def study_session(df):\n",
        "    question_times = 0 # Number of Questions\n",
        "\n",
        "    while True: # Loop for the entire session, continues until user quits\n",
        "        # MODIFIED: Call choose_QnA_mode first\n",
        "        qna_mode = choose_QnA_mode()\n",
        "        topic = choose_topic(df)\n",
        "\n",
        "        # Start of a new topic round\n",
        "        asked_indices_this_topic = set()\n",
        "        correct_answers_this_topic = 0\n",
        "        questions_asked_this_topic = 0\n",
        "\n",
        "        for i in range(max_questions_topic):\n",
        "            question_times += 1\n",
        "\n",
        "            # MODIFIED: Pass the qna_mode to the function\n",
        "            idx, is_correct = interactive_question_answering(df, topic, asked_indices_this_topic, question_times, qna_mode)\n",
        "\n",
        "            # MODIFIED: Bug fix for when no questions are left\n",
        "            if idx is None:\n",
        "                break\n",
        "\n",
        "            asked_indices_this_topic.add(idx)\n",
        "            questions_asked_this_topic += 1\n",
        "\n",
        "            if is_correct:\n",
        "               correct_answers_this_topic += 1\n",
        "\n",
        "        # Topic Round-up and Mastery Check\n",
        "        if questions_asked_this_topic > 0:\n",
        "            score_percentage = correct_answers_this_topic / questions_asked_this_topic\n",
        "            print(f\"\\n--- Topic Summary for '{topic}' ({qna_mode} Mode) ---\") # MODIFIED: Added mode to summary\n",
        "            print(f\"You answered {correct_answers_this_topic} out of {questions_asked_this_topic} questions correctly.\")\n",
        "\n",
        "            if score_percentage >= mastery_threshold:\n",
        "                print(\"Congratulations! You have a good grasp of this topic.\")\n",
        "            else:\n",
        "                print(\"You're making progress! A little more practice on this topic would be helpful.\")\n",
        "\n",
        "        # Ask user if they want to continue\n",
        "        another_round = input(\"\\nWould you like to choose another topic or mode? (yes/no): \").strip().lower()\n",
        "        if another_round != 'yes':\n",
        "            break\n",
        "\n",
        "    print(\"\\nStudy session finished. Great work!\")\n",
        "\n",
        "# Assuming `df` is already loaded from your CSV file in a previous step\n",
        "# For example:\n",
        "# import pandas as pd\n",
        "# df = pd.read_csv(\"path/to/your/file.csv\")\n",
        "# df.columns = [col.strip().replace('\"', '').replace('\\n', '') for col in df.columns] # Clean column names\n",
        "\n",
        "study_session(df)\n"
      ],
      "metadata": {
        "id": "SCFl34Gv5NV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GNt_27v2S6kv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}